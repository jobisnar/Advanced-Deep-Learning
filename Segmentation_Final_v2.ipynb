{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Segmentation Final v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BB_Bhpig-Oo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "d565fc73-b315-4dfd-a3de-ba9d37c3feac"
      },
      "source": [
        "#Link to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "\n",
        "#segmentation folder contains data, weights folder\n",
        "#data contains Drinks dataset folder\n",
        "os.chdir(\"drive/My Drive/Segmentation\")\n",
        "os.listdir()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['data',\n",
              " 'prediction_1',\n",
              " 'input_image.png',\n",
              " 'segmentation.png',\n",
              " 'background.png',\n",
              " 'prediction_final',\n",
              " 'prediction',\n",
              " 'miou_history.npy',\n",
              " 'mpla_history.npy',\n",
              " 'weights']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyGppbSHg8UP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import skimage\n",
        "\n",
        "from skimage.io import imread\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "class DrinksDataset(Dataset):\n",
        "    def __init__(self, label_file, root_dir, batch_size = 1, shuffle = True, transform = None):\n",
        "        \n",
        "        self.label_file = label_file\n",
        "        self.root_dir = root_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.get_dictionary()\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __len__(self):\n",
        "        \"\"\"Get total number of batches\"\"\"\n",
        "        length = np.floor(len(self.dictionary) / self.batch_size)\n",
        "        return int(length)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Get a batch of data\"\"\"\n",
        "        start_index = index * self.batch_size\n",
        "        end_index = (index+1) * self.batch_size\n",
        "        keys = self.keys[start_index : end_index]\n",
        "        x, y = self.__data_generation(keys)\n",
        "        return x, y\n",
        "    \n",
        "    def get_dictionary(self):\n",
        "        \"\"\"Load ground truth dictionary of \n",
        "            image filename : segmentation masks\n",
        "        \"\"\"\n",
        "        path = os.path.join(self.root_dir,\n",
        "                            self.label_file)\n",
        "        self.dictionary = np.load(path,\n",
        "                                  allow_pickle=True).flat[0]\n",
        "        self.keys = np.array(list(self.dictionary.keys()))\n",
        "        labels = self.dictionary[self.keys[0]]\n",
        "        self.n_classes = labels.shape[-1]\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.keys)\n",
        "\n",
        "\n",
        "    def __data_generation(self, keys):\n",
        "        \"\"\"Generate train data: images and \n",
        "        segmentation ground truth labels \n",
        "\n",
        "        Arguments:\n",
        "            keys (array): Randomly sampled keys\n",
        "                (key is image filename)\n",
        "\n",
        "        Returns:\n",
        "            x (tensor): Batch of images\n",
        "            y (tensor): Batch of pixel-wise categories\n",
        "        \"\"\"\n",
        "        # a batch of images\n",
        "        x = []\n",
        "        # and their corresponding segmentation masks\n",
        "        y = []\n",
        "\n",
        "        for i, key in enumerate(keys):\n",
        "            # images are assumed to be stored \n",
        "            # in self.data_path\n",
        "            # key is the image filename\n",
        "            \n",
        "            image_path = os.path.join(self.root_dir, key)\n",
        "            image = skimage.img_as_float(imread(image_path))\n",
        "            # append image to the list\n",
        "            x.append(image)\n",
        "            # and its corresponding label (segmentation mask)\n",
        "            labels = self.dictionary[key]\n",
        "            y.append(labels)\n",
        "\n",
        "        return np.array(x), np.array(y)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMzfaVOCg8VQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class resnet_layer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels = 3,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=False):\n",
        "        \n",
        "        super(resnet_layer, self).__init__()\n",
        "        \n",
        "        #set attributes\n",
        "        self.conv_first = conv_first\n",
        "        self.batch_normalization = batch_normalization\n",
        "        self.activation = activation\n",
        "        \n",
        "        self.padding = int((kernel_size-1)/2)\n",
        "        self.channels = num_filters\n",
        "        \n",
        "        #conv layer\n",
        "        self.conv = nn.Conv2d(in_channels = in_channels,\n",
        "                              out_channels = num_filters,\n",
        "                              kernel_size = kernel_size,\n",
        "                              stride = strides,\n",
        "                              padding = self.padding\n",
        "                              )\n",
        "        \n",
        "        #he normal initialization\n",
        "        nn.init.kaiming_normal_(self.conv.weight) \n",
        "        \n",
        "        #batch norm\n",
        "        self.bn = nn.BatchNorm2d(in_channels)\n",
        "        \n",
        "        #change batch norm if conv_first is True\n",
        "        if self.conv_first:\n",
        "            self.bn = nn.BatchNorm2d(num_filters)\n",
        "    \n",
        "    #forward code\n",
        "    def forward(self, x):\n",
        "        if self.conv_first:\n",
        "            x = self.conv(x)\n",
        "            if self.batch_normalization:\n",
        "                x = self.bn(x)\n",
        "            if self.activation is not None:\n",
        "                if self.activation == 'relu':\n",
        "                    x = F.relu(x)\n",
        "        else:\n",
        "            if self.batch_normalization:\n",
        "                x = self.bn(x)\n",
        "            if self.activation is not None:\n",
        "                if self.activation == 'relu':\n",
        "                    x = F.relu(x)\n",
        "            x = self.conv(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVTZ9-Qbg8Vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class resnet_v1(nn.Module):\n",
        "    def __init__(self, in_channels, depth, n_layers=4):\n",
        "        super(resnet_v1, self).__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.depth = depth\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        if (depth - 2) % 6 != 0:\n",
        "            raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "        # Start model definition\n",
        "        self.num_filters = 16\n",
        "        self.num_res_blocks = int((depth - 2) / 6)\n",
        "        \n",
        "        #List of resnet, allows to be seen as modules\n",
        "        self.res_x = nn.ModuleList()\n",
        "        self.res_y = nn.ModuleList()\n",
        "        \n",
        "        #layers\n",
        "        self.res_x.append(resnet_layer(in_channels = in_channels))\n",
        "        \n",
        "        # Instantiate the stack of residual units\n",
        "        for stack in range(3):\n",
        "            for res_block in range(self.num_res_blocks):\n",
        "                strides = 1\n",
        "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                    strides = 2  # downsample\n",
        "                    \n",
        "                #in channel = out channel of previous layer\n",
        "                self.res_y.append(resnet_layer(in_channels=self.res_x[len(self.res_x)-1].channels,\n",
        "                                               num_filters=self.num_filters,\n",
        "                                               strides=strides\n",
        "                                              ))\n",
        "                \n",
        "                #in channel = out channel of previous layer\n",
        "                self.res_y.append(resnet_layer(in_channels=self.res_y[len(self.res_y)-1].channels,\n",
        "                                               num_filters=self.num_filters,\n",
        "                                               activation=None\n",
        "                                              ))\n",
        "                \n",
        "                \n",
        "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                    # linear projection residual shortcut connection to match\n",
        "                    # changed dims\n",
        "                    self.res_x.append(resnet_layer(in_channels=self.res_x[len(self.res_x)-1].channels,\n",
        "                                                   num_filters=self.num_filters,\n",
        "                                                   kernel_size=1,\n",
        "                                                   strides=strides,\n",
        "                                                   activation=None,\n",
        "                                                   batch_normalization=False))\n",
        "                    \n",
        "                    \n",
        "        self.feat_pyramid = features_pyramid(in_channels = self.num_filters,\n",
        "                                            n_layers = self.n_layers\n",
        "                                           )\n",
        "        \n",
        "        #can be called by other functions\n",
        "        self.out_channels = self.num_filters\n",
        "        \n",
        "    def forward(self, x):    \n",
        "\n",
        "        #making resnet indices\n",
        "        x_ind = -1\n",
        "        y_ind = -1\n",
        "        \n",
        "        x_ind += 1\n",
        "        x = self.res_x[x_ind](x)\n",
        "        #print(\"x.shape:\", x.shape)\n",
        "         \n",
        "        # Instantiate the stack of residual units\n",
        "        for stack in range(3):\n",
        "            #print(\"stack:\", stack)\n",
        "            for res_block in range(self.num_res_blocks):\n",
        "                y_ind += 1\n",
        "                y = self.res_y[y_ind](x)\n",
        "                #print(\"y.shape:\", y.shape)\n",
        "                \n",
        "                y_ind += 1\n",
        "                y = self.res_y[y_ind](y)\n",
        "                #print(\"y.shape:\", y.shape)\n",
        "                \n",
        "                \n",
        "                if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                    # linear projection residual shortcut connection to match\n",
        "                    # changed dims\n",
        "                    x_ind += 1\n",
        "                    x = self.res_x[x_ind](x)\n",
        "                    #print(\"x.shape:\", x.shape)\n",
        "                \n",
        "                #x and y went through diff. paths\n",
        "                x = x.add(y)\n",
        "                x = F.relu(x)\n",
        "                \n",
        "            self.num_filters *= 2\n",
        "\n",
        "        # feature maps\n",
        "        outputs = self.feat_pyramid(x) #replaced num_classes with n_layers\n",
        "\n",
        "        return outputs #not x?\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CF6UrGv1g8Vw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class features_pyramid(nn.Module):\n",
        "    def __init__(self,in_channels = 3, n_layers=4):\n",
        "        super(features_pyramid, self).__init__()\n",
        "        \n",
        "        \"\"\"Generate features pyramid from the output of the \n",
        "        last layer of a backbone network (e.g. ResNetv1 or v2)\n",
        "\n",
        "        Arguments:\n",
        "            x (tensor): Output feature maps of a backbone network\n",
        "            n_layers (int): Number of additional pyramid layers\n",
        "\n",
        "        Return:\n",
        "            outputs (list): Features pyramid \n",
        "        \"\"\"\n",
        "        \n",
        "        n_filters = 512\n",
        "        channels = in_channels\n",
        "        \n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2)\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        # additional feature map layers\n",
        "        for i in range(self.n_layers - 1):\n",
        "            self.layers.append(conv_layer(in_channels = channels,\n",
        "                                          num_filters = n_filters,\n",
        "                                          kernel_size=3,\n",
        "                                          strides=2,\n",
        "                                          use_maxpool=False\n",
        "                                         ))\n",
        "            channels = n_filters\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "    def forward(self, x):\n",
        "        outputs = [x]\n",
        "        conv = self.avgpool(x)\n",
        "        outputs.append(conv)\n",
        "        prev_conv = conv\n",
        "\n",
        "        # additional feature map layers\n",
        "        for i in range(self.n_layers - 1):\n",
        "\n",
        "            conv = self.layers[i](prev_conv)\n",
        "            \n",
        "            outputs.append(conv)\n",
        "            prev_conv = conv\n",
        "            \n",
        "        return outputs\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGdpoEK8g8WK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class conv_layer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels = 3,\n",
        "                 num_filters=32,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 use_maxpool=True,\n",
        "                 activation='relu',\n",
        "                ):\n",
        "        \n",
        "        super(conv_layer, self).__init__()\n",
        "        \n",
        "        #set attributes\n",
        "        self.in_channels = in_channels\n",
        "        self.activation = activation\n",
        "        self.use_maxpool = use_maxpool\n",
        "        self.padding = int((kernel_size-1)/2)\n",
        "        \n",
        "        self.conv = nn.Conv2d(in_channels = self.in_channels, \n",
        "                      out_channels = num_filters,\n",
        "                      kernel_size = kernel_size,\n",
        "                      stride = strides,\n",
        "                      padding = self.padding #p = (f-1)/2\n",
        "                     )\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(num_filters)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 2)\n",
        "        \n",
        "        #HE NORMAL INITIALIZATION\n",
        "        nn.init.kaiming_normal_(self.conv.weight) \n",
        "    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        if self.activation == 'relu':\n",
        "            x = F.relu(x)\n",
        "        if self.use_maxpool:\n",
        "            x = self.maxpool(x)\n",
        "        return x"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy7JAE6-g8Vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class resnet_v2(nn.Module):\n",
        "    def __init__(self, in_channels, depth, n_layers=4):\n",
        "        super(resnet_v2, self).__init__()\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "        self.depth = depth\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        if (self.depth - 2) % 9 != 0:\n",
        "            raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "        # Start model definition.\n",
        "        num_filters_in = 16\n",
        "        num_filters_out = 0\n",
        "        self.num_res_blocks = int((self.depth - 2) / 9)\n",
        "\n",
        "        #List of resnet, allows to be seen as modules\n",
        "        self.res_x = nn.ModuleList()\n",
        "        self.res_y = nn.ModuleList()\n",
        "        \n",
        "        #LAYERS\n",
        "        self.res_x.append(resnet_layer(in_channels = in_channels,\n",
        "                                      num_filters = num_filters_in,\n",
        "                                      conv_first = True\n",
        "                                      ))\n",
        "        \n",
        "        # Instantiate the stack of residual units\n",
        "        for stage in range(3):\n",
        "            for res_block in range(self.num_res_blocks):\n",
        "                activation = 'relu'\n",
        "                batch_normalization = True\n",
        "                strides = 1\n",
        "                if stage == 0:\n",
        "                    num_filters_out = num_filters_in * 4\n",
        "                    if res_block == 0:  # first layer and first stage\n",
        "                        activation = None\n",
        "                        batch_normalization = False\n",
        "                else:\n",
        "                    num_filters_out = num_filters_in * 2\n",
        "                    if res_block == 0:  # first layer but not first stage\n",
        "                        strides = 2    # downsample\n",
        "                        \n",
        "                        \n",
        "                # bottleneck residual unit\n",
        "                self.res_y.append(resnet_layer(in_channels=self.res_x[len(self.res_x)-1].channels,\n",
        "                                               num_filters=num_filters_in,\n",
        "                                               kernel_size = 1,\n",
        "                                               strides=strides,\n",
        "                                               activation=activation,\n",
        "                                               batch_normalization=batch_normalization,\n",
        "                                               conv_first=False\n",
        "                                              ))\n",
        "                \n",
        "                #in channel = out channel of previous layer\n",
        "                self.res_y.append(resnet_layer(in_channels=self.res_y[len(self.res_y)-1].channels,\n",
        "                                               num_filters=num_filters_in,\n",
        "                                               conv_first=False\n",
        "                                              ))\n",
        "                \n",
        "                self.res_y.append(resnet_layer(in_channels=self.res_y[len(self.res_y)-1].channels,\n",
        "                                               num_filters=num_filters_out,\n",
        "                                               kernel_size=1,\n",
        "                                               conv_first=False\n",
        "                                              ))\n",
        "                if res_block == 0:\n",
        "                    # linear projection residual shortcut connection to match\n",
        "                    # changed dims\n",
        "                    x = self.res_x.append(resnet_layer(in_channels=self.res_x[len(self.res_x)-1].channels,\n",
        "                                                       num_filters=num_filters_out,\n",
        "                                                       kernel_size=1,\n",
        "                                                       strides=strides,\n",
        "                                                       activation=None,\n",
        "                                                       batch_normalization=False\n",
        "                                                      ))\n",
        "\n",
        "            num_filters_in = num_filters_out\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(num_filters_out)\n",
        "        \n",
        "        self.feat_pyramid = features_pyramid(in_channels = num_filters_out,\n",
        "                                            n_layers = self.n_layers\n",
        "                                           )\n",
        "        #can be called by other functions\n",
        "        self.out_channels = num_filters_out\n",
        "    \n",
        "    def forward(self, x):    \n",
        "        \n",
        "        #making resnet indices\n",
        "        x_ind = -1\n",
        "        y_ind = -1\n",
        "        \n",
        "        \n",
        "        # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "        x_ind += 1\n",
        "        x = self.res_x[x_ind](x)\n",
        "        #print(\"x.shape:\", x.shape)\n",
        "\n",
        "        \n",
        "        # Instantiate the stack of residual units\n",
        "        for stage in range(3):\n",
        "            for res_block in range(self.num_res_blocks):\n",
        "                \n",
        "                # bottleneck residual unit\n",
        "                y_ind += 1  \n",
        "                y = self.res_y[y_ind](x)\n",
        "                #print(\"y.shape:\", y.shape)\n",
        "                \n",
        "                y_ind += 1\n",
        "                y = self.res_y[y_ind](y)\n",
        "                #print(\"y.shape:\", y.shape)\n",
        "                \n",
        "                y_ind += 1\n",
        "                y = self.res_y[y_ind](y)\n",
        "                #print(\"y.shape:\", y.shape)\n",
        "                \n",
        "                if res_block == 0:\n",
        "                    # linear projection residual shortcut connection to match\n",
        "                    # changed dims\n",
        "                    x_ind += 1\n",
        "                    x = self.res_x[x_ind](x)\n",
        "                    #print(\"x.shape:\", x.shape)\n",
        "                \n",
        "                x = x.add(y) #add 2 outputs together\n",
        "\n",
        "        # v2 has BN-ReLU before Pooling\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "        # 1st feature map layer\n",
        "\n",
        "        # main feature maps (160, 120)\n",
        "        # succeeding feature maps are scaled down by\n",
        "        # 2, 4, 8\n",
        "        outputs = self.feat_pyramid(x)\n",
        "        \n",
        "        return outputs #returns list of feature maps"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kp_eWecag8V-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_resnet(in_channels=3,\n",
        "                 n_layers=4,\n",
        "                 version=1,\n",
        "                 n=6):\n",
        "    \"\"\"Build a resnet as backbone\n",
        "\n",
        "    # Arguments:\n",
        "        input_shape (list): Input image size and channels\n",
        "        n_layers (int): Number of feature layers \n",
        "        version (int): Supports ResNetv1 and v2 but v2 by default\n",
        "        n (int): Determines number of ResNet layers\n",
        "                 (Default is ResNet50)\n",
        "\n",
        "    # Returns\n",
        "        model (Keras Model)\n",
        "\n",
        "    \"\"\"\n",
        "    # computed depth from supplied model parameter n\n",
        "    if version == 1:\n",
        "        depth = n * 6 + 2\n",
        "    elif version == 2:\n",
        "        depth = n * 9 + 2\n",
        "\n",
        "    # model name, depth and version\n",
        "    # input_shape (h, w, 3)\n",
        "    if version==1:\n",
        "        model = resnet_v1(in_channels=in_channels,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    else:\n",
        "        model = resnet_v2(in_channels=in_channels,\n",
        "                          depth=depth,\n",
        "                          n_layers=n_layers)\n",
        "    \n",
        "    #RETURNS RESNET MODEL IN PYTORCH\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITra8923g8WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class tconv_layer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels = 3,\n",
        "                 num_filters=32,\n",
        "                 kernel_size=3,\n",
        "                 strides=2\n",
        "                ):\n",
        "        \"\"\"Helper function to build Conv2DTranspose-BN-ReLU \n",
        "            layer\n",
        "        \"\"\"\n",
        "        super(tconv_layer, self).__init__()\n",
        "        \n",
        "        self.padding = int((kernel_size-1)/2)\n",
        "        \n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.tconv = nn.ConvTranspose2d(in_channels = self.in_channels, \n",
        "                                        out_channels=num_filters,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        stride=strides,\n",
        "                                        padding=self.padding,\n",
        "                                        output_padding = 1\n",
        "                                       )\n",
        "        \n",
        "        #HE NORMAL INITIALIZATION\n",
        "        nn.init.kaiming_normal_(self.tconv.weight)\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(num_filters)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.tconv(x)\n",
        "        x = self.bn(x)\n",
        "        x = F.relu(x)\n",
        "            \n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOBBctngg8WT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class build_fcn(nn.Module):\n",
        "    def __init__(self,\n",
        "                 backbone,\n",
        "                 n_classes=4\n",
        "                ):\n",
        "        \"\"\"Helper function to build Conv2DTranspose-BN-ReLU \n",
        "            layer\n",
        "        \"\"\"\n",
        "        super(build_fcn, self).__init__()\n",
        "        \n",
        "        self.backbone = backbone\n",
        "        \n",
        "    \n",
        "        #can be called by other functions\n",
        "        channels = backbone.out_channels\n",
        "        \n",
        "        self.layers = nn.ModuleList()\n",
        "        self.upsample = nn.ModuleList()\n",
        "        \n",
        "        size = 2\n",
        "        \n",
        "        for i in range(backbone.n_layers):\n",
        "            if i>0:\n",
        "                channels = 512\n",
        "                \n",
        "            self.layers.append(conv_layer(channels,\n",
        "                                 num_filters=256,\n",
        "                                 use_maxpool=False))\n",
        "\n",
        "            #used scale_factor instead of size in keras\n",
        "            self.upsample.append(nn.Upsample(scale_factor=size, mode='bilinear', align_corners = False))\n",
        "\n",
        "            size = size * 2\n",
        "            \n",
        "            \n",
        "        tconv_in_channel = 256 * backbone.n_layers + backbone.out_channels\n",
        "        \n",
        "        self.tconv1 = tconv_layer(in_channels = tconv_in_channel, num_filters = 256)\n",
        "        self.tconv2 = tconv_layer(in_channels = 256, num_filters = 256)\n",
        "        \n",
        "        self.tconv3 = nn.ConvTranspose2d(in_channels = 256, \n",
        "                                        out_channels = n_classes,\n",
        "                                        kernel_size = 1,\n",
        "                                        stride = 1,\n",
        "                                        padding = 0\n",
        "                                       )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        features = self.backbone(x)\n",
        "        main_feature = features[0]\n",
        "        features = features[1:]\n",
        "        out_features = [main_feature]\n",
        "        \n",
        "        for i, feature in enumerate(features):\n",
        "            feature = self.layers[i](feature)\n",
        "\n",
        "            #used scale_factor instead of size in keras\n",
        "            feature = self.upsample[i](feature)\n",
        "\n",
        "            out_features.append(feature)\n",
        "        \n",
        "        # concatenate all upsampled features\n",
        "        x = torch.cat(out_features, 1) #along which axes to concatenate?\n",
        "        #print(\"cat:\", x.shape)\n",
        "        # perform 2 additional feature extraction \n",
        "        # and upsampling\n",
        "        x = self.tconv1(x)\n",
        "        #print(\"tconv1:\", x.shape)\n",
        "        x = self.tconv2(x)        \n",
        "        #print(\"tconv2:\", x.shape)\n",
        "        \n",
        "        #pixel-wise classifier\n",
        "        x = self.tconv3(x)\n",
        "        #print(\"tconv3:\", x.shape)\n",
        "        #x = nn.Softmax(dim=1)(x)\n",
        "        \n",
        "        \n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLaWAbJTg8WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "class FCN(nn.Module):\n",
        "    \"\"\"Made of an fcn model and a dataset generator.\n",
        "    Define functions to train and validate an FCN model.\n",
        "\n",
        "    Attributes:\n",
        "        fcn (model): FCN network model\n",
        "        train_generator: Multi-threaded \n",
        "            data generator for training\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self, restore_weights_path = \"\", label_file = \"segmentation_train.npy\", \n",
        "                 n_layers = 3, backbone = build_resnet, \n",
        "                 transform = None, image_file = None,\n",
        "                 test_labels = \"segmentation_test.npy\",\n",
        "                 root_dir = \"data/drinks\", batch_size = 1, shuffle = True,\n",
        "                 dataset = \"drinks\", epochs = 5, plot = True,\n",
        "                 in_channels = 3, save_dir = \"weights\",\n",
        "                 evaluate = False, train = False, save = False, device = 'cpu'\n",
        "                ):\n",
        "        super(FCN, self).__init__()\n",
        "            \n",
        "        \"\"\"Copy user-defined configs.\n",
        "        Build backbone and fcn network models.\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.backbone_net = backbone\n",
        "        self.n_layers = n_layers\n",
        "        self.save_dir = save_dir\n",
        "        self.root_dir = root_dir\n",
        "        self.test_labels = test_labels\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset\n",
        "        self.epochs = epochs\n",
        "        self.restore_weights_path = restore_weights_path\n",
        "        self.image_file = image_file\n",
        "        self.plot = plot\n",
        "        self.save = save\n",
        "        self.device = device\n",
        "\n",
        "        #initialize total loss\n",
        "        self.total_loss = 0\n",
        "\n",
        "        #initialize epoch\n",
        "        self.curr_epoch = -1\n",
        "        \n",
        "        \n",
        "        if transform is None:\n",
        "            transform = transforms.ToTensor()\n",
        "        \n",
        "        \n",
        "        self.fcn = None\n",
        "        self.train_generator = DrinksDataset(label_file, root_dir, batch_size, \n",
        "                                             shuffle, transform)\n",
        "        self.build_model()\n",
        "        self.eval_init()\n",
        "        \n",
        "        \"\"\"set these arguments to set what to do with the model.\"\"\"\n",
        "        if restore_weights_path != \"\":\n",
        "            self.restore_weights()\n",
        "\n",
        "        if train:\n",
        "            self.train_network()\n",
        "\n",
        "        if evaluate:\n",
        "            if image_file is None:\n",
        "                self.eval()\n",
        "            else:\n",
        "                self.evaluate()\n",
        "        \n",
        "\n",
        "\n",
        "    def build_model(self):\n",
        "        \"\"\"Build a backbone network and use it to\n",
        "            create a semantic segmentation \n",
        "            network based on FCN.\n",
        "        \"\"\"\n",
        "        \n",
        "        # input shape is (3, 480, 640) by default\n",
        "        \n",
        "\n",
        "        # build the backbone network (eg ResNet50)\n",
        "        # the backbone is used for 1st set of features\n",
        "        # of the features pyramid\n",
        "        self.backbone = self.backbone_net(self.in_channels,\n",
        "                                          n_layers=self.n_layers)\n",
        "\n",
        "        # using the backbone, build fcn network\n",
        "        # output layer is a pixel-wise classifier\n",
        "        #n_classes exists in train_generator\n",
        "        self.n_classes =  self.train_generator.n_classes\n",
        "        \n",
        "        self.fcn = build_fcn(backbone = self.backbone, \n",
        "                             n_classes = self.n_classes\n",
        "                            )\n",
        "        \n",
        "        print(\"Modules: \", self.fcn.modules)\n",
        "        \n",
        "        self.optimizer = optim.Adam(self.fcn.parameters(), lr = 0.001)\n",
        "\n",
        "    def eval_init(self):\n",
        "        \"\"\"Housekeeping for trained model evaluation\"\"\"\n",
        "        # model weights are saved for future validation\n",
        "        # prepare model model saving directory.\n",
        "        save_dir = os.path.join(os.getcwd(), self.save_dir)\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "                \n",
        "        model_name = str(self.n_layers) + \"layer-\"\n",
        "        model_name += self.dataset\n",
        "        model_name += '-best-iou.h5'\n",
        "        log = \"Weights filename: %s\" % model_name\n",
        "        print(log)\n",
        "        self.weights_path = os.path.join(save_dir, model_name)\n",
        "        self.preload_test()\n",
        "        self.miou = 0\n",
        "        self.miou_history = []\n",
        "        self.mpla_history = []\n",
        "\n",
        "\n",
        "    def preload_test(self):\n",
        "        \"\"\"Pre-load test dataset to save time \"\"\"\n",
        "        path = os.path.join(self.root_dir,\n",
        "                            self.test_labels)\n",
        "\n",
        "        # ground truth data is stored in an npy file\n",
        "        self.test_dictionary = np.load(path,\n",
        "                                       allow_pickle=True).flat[0]\n",
        "        self.test_keys = np.array(list(self.test_dictionary.keys()))\n",
        "        print(\"Loaded %s\" % path)\n",
        "        \n",
        "   \n",
        "    #CHECK TRAIN ON OTHER PYTORCH CODE            \n",
        "    def train_network(self):\n",
        "        \"\"\"Train an FCN\"\"\"\n",
        "        log = \"# of classes %d\" % self.n_classes\n",
        "        print(log)\n",
        "        log = \"Batch size: %d\" % self.batch_size\n",
        "        print(log)\n",
        "\n",
        "        # prepare callbacks for saving model weights\n",
        "        # and learning rate scheduler\n",
        "        # model weights are saved when test iou is highest\n",
        "\n",
        "        # learning rate decreases by 50% every 20 epochs\n",
        "        # after 40th epoch        \n",
        "\n",
        "        \n",
        "        #GET PYTORCH LOOP\n",
        "        #use .to('cuda') for network, images, labels to run on cuda.\n",
        "        device = self.device\n",
        "\n",
        "        #RUN ON CUDA\n",
        "        self.fcn.to(device)\n",
        "        \n",
        "        \"\"\"optimizer is in self.optimizer, adam\"\"\"\n",
        "\n",
        "        torch.set_grad_enabled(True)\n",
        "        \n",
        "        #SET MODEL INTO TRAINING MODE\n",
        "        self.fcn.train()\n",
        "\n",
        "        print(\"Training network....\")\n",
        "        \n",
        "        print(\"data length: \" + str(len(self.train_generator)))\n",
        "        if self.curr_epoch != -1:\n",
        "            print(\"Epochs done:\", self.curr_epoch)\n",
        "\n",
        "        total_loss = self.total_loss\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            #skip until current epoch if weights are loaded\n",
        "            if self.curr_epoch != -1 and epoch < self.curr_epoch:\n",
        "                    continue\n",
        "\n",
        "            #LR scheduler\n",
        "            if(epoch > 38 and epoch < 82 and (epoch+1) % 20 == 0):\n",
        "                for param in self.optimizer.param_groups:\n",
        "                    temp = param['lr']\n",
        "                    param['lr'] = temp/2\n",
        "                print(\"epoch:\", (epoch+1),\"new lr:\", param['lr'])\n",
        "          \n",
        "            t0 = time.perf_counter()\n",
        "\n",
        "            for i, batch in enumerate(self.train_generator):\n",
        "                if i == len(self.train_generator):\n",
        "                    break\n",
        "                \n",
        "                images = torch.from_numpy(batch[0]).permute(0, 3, 1, 2).float().to(device) #RUN ON CUDA\n",
        "                labels = torch.from_numpy(batch[1]).permute(0, 3, 1, 2).float() \n",
        "                \n",
        "                preds = self.fcn(images)\n",
        "\n",
        "                loss_func = torch.nn.CrossEntropyLoss()\n",
        "                \n",
        "                targets = np.argmax(labels, axis = 1).to(device) #RUN ON CUDA\n",
        "                \n",
        "                loss = loss_func(preds, targets)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                print(\"batch:\", i)\n",
        "                self.batch = i\n",
        "                print(\"time elapsed: \", time.perf_counter()-t0)\n",
        "                print(\"total_loss:\", total_loss)\n",
        "                print(\"loss:\", loss.item())\n",
        "\n",
        "            #remove when loss is fixed\n",
        "            self.total_loss = total_loss\n",
        "            print(\"epoch:\", epoch + 1, \"out of\", self.epochs, \"done, total_loss:\", total_loss)\n",
        "            print(\"time elapsed: \", time.perf_counter()-t0)\n",
        "\n",
        "        print(\"Done training!\")\n",
        "        if self.save == True:\n",
        "                print(\"Saving weights... %s\" % self.weights_path)\n",
        "                #SAVE WEIGHTS FUNCTION FOR PYTORCH\n",
        "                torch.save({\n",
        "                    'epoch': (epoch+1),\n",
        "                    'model_state_dict': self.fcn.state_dict(),\n",
        "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                    'loss': self.total_loss\n",
        "                }, self.weights_path)\n",
        "      \n",
        "        print(\"Weights are saved.\")\n",
        "\n",
        "\n",
        "\n",
        "    def restore_weights(self):\n",
        "        \"\"\"Load previously trained model weights\"\"\"\n",
        "        #need to enter weight filename in restore_weights\n",
        "        if self.restore_weights_path != \"\":\n",
        "            save_dir = os.path.join(os.getcwd(), self.save_dir)\n",
        "            filename = os.path.join(save_dir, self.restore_weights_path)\n",
        "            log = \"Loading weights: %s\" % filename\n",
        "            print(log)\n",
        "\n",
        "            checkpoint = torch.load(filename)\n",
        "            self.fcn.load_state_dict(checkpoint['model_state_dict'])\n",
        "            self.fcn = self.fcn.to(self.device)\n",
        "\n",
        "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            \n",
        "            #IMPLEMENT CURR EPOCH AND TOTAL LOSS IN TRAINING\n",
        "            self.curr_epoch = checkpoint['epoch']\n",
        "            self.total_loss = checkpoint['loss']\n",
        "\n",
        "            print(\"Restored weights.\")\n",
        "         \n",
        "        \n",
        "    def to_categorical(self, y, num_classes):\n",
        "        return np.eye(num_classes, dtype=y.dtype)[y]\n",
        "\n",
        "    def segment_objects(self, image, normalized=True):\n",
        "        \"\"\"Run segmentation prediction for a given image\n",
        "    \n",
        "        Arguments:\n",
        "            image (tensor): Image loaded in a numpy tensor.\n",
        "                RGB components range is [0.0, 1.0]\n",
        "            normalized (Bool): Use normalized=True for \n",
        "                pixel-wise categorical prediction. False if \n",
        "                segmentation will be displayed in RGB\n",
        "                image format.\n",
        "        \"\"\"\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        image = torch.from_numpy(image).float().permute(0, 3, 1, 2).to(self.device)\n",
        "\n",
        "        segmentation = self.fcn(image)\n",
        "        segmentation = segmentation.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
        "        segmentation = np.squeeze(segmentation, axis=0)\n",
        "        segmentation = np.argmax(segmentation, axis=-1)\n",
        "        segmentation = self.to_categorical(segmentation, self.n_classes)\n",
        "        if not normalized:\n",
        "            segmentation = segmentation * 255\n",
        "        segmentation = segmentation.astype('uint8')\n",
        "        return segmentation\n",
        "\n",
        "\n",
        "    def evaluate(self, imagefile=None, image=None):\n",
        "        \"\"\"Perform segmentation on a given image filename\n",
        "            and display the results.\n",
        "        \"\"\"\n",
        "        import matplotlib.pyplot as plt\n",
        "        save_dir = \"prediction\"\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "            \n",
        "            \n",
        "            \n",
        "        #CHANGED IMAGE WITH IMAGEFILE\n",
        "        if image is not None:\n",
        "            #splitext separates filename and extension, 0 is filename w/ root\n",
        "            imagefile = os.path.splitext(imagefile)[0]\n",
        "        elif self.image_file is not None:\n",
        "            image = skimage.img_as_float(imread(self.image_file))\n",
        "            imagefile = os.path.split(self.image_file)[-1]\n",
        "            imagefile = os.path.splitext(imagefile)[0]\n",
        "            print(\"imagefile:\", imagefile)\n",
        "        else:\n",
        "            raise ValueError(\"Image file must be known\")\n",
        "\n",
        "        maskfile =imagefile + \"-mask-epoch-\"+str(self.epochs)+\".png\"\n",
        "        mask_path = os.path.join(save_dir, maskfile)\n",
        "        inputfile = imagefile + \"-input.png\"\n",
        "        input_path = os.path.join(save_dir, inputfile)\n",
        "\n",
        "        #image = torch.from_numpy(image).float().to(self.device)\n",
        "\n",
        "        segmentation = self.segment_objects(image,\n",
        "                                            normalized=False)\n",
        "        mask = segmentation[..., 1:]\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title('Input image', fontsize=14)\n",
        "        plt.imshow(image)\n",
        "        plt.savefig(input_path)\n",
        "        #plt.show()\n",
        "\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title('Semantic segmentation', fontsize=14)\n",
        "        plt.imshow(mask)\n",
        "        plt.savefig(mask_path)\n",
        "        #plt.show()\n",
        "        \n",
        "    def eval(self):\n",
        "        \"\"\"Evaluate a trained FCN model using mean IoU\n",
        "            metric.\n",
        "        \"\"\"\n",
        "        print(\"Evaluating..\")\n",
        "        \n",
        "        s_iou = 0\n",
        "        s_pla = 0\n",
        "        # evaluate iou per test image\n",
        "        eps = np.finfo(float).eps\n",
        "        for i, key in enumerate(self.test_keys):\n",
        "            # load a test image\n",
        "            image_path = os.path.join(self.root_dir, key)\n",
        "            print(\"Loading img:\", key)\n",
        "            \n",
        "            image = skimage.img_as_float(imread(image_path))\n",
        "            segmentation = self.segment_objects(image) \n",
        "            # load test image ground truth labels\n",
        "            gt = self.test_dictionary[key]\n",
        "            i_pla = 100 * (gt == segmentation).all(axis=(2)).mean()\n",
        "            s_pla += i_pla\n",
        "            \n",
        "            i_iou = 0\n",
        "            n_masks = 0\n",
        "            # compute mask for each object in the test image\n",
        "            # including background\n",
        "            for i in range(self.n_classes):\n",
        "                if np.sum(gt[..., i]) < eps: \n",
        "                    continue\n",
        "                mask = segmentation[..., i]\n",
        "                intersection = mask * gt[..., i]\n",
        "                union = np.ceil((mask + gt[..., i]) / 2.0)\n",
        "                intersection = np.sum(intersection) \n",
        "                union = np.sum(union) \n",
        "                if union > eps:\n",
        "                    iou = intersection / union\n",
        "                    #print(\"iou of\", n_masks, \"is\", iou)\n",
        "                    i_iou += iou\n",
        "                    n_masks += 1\n",
        "            \n",
        "            # average iou per image\n",
        "            i_iou /= n_masks\n",
        "            log = \"%s: %d objs, miou=%0.4f ,pla=%0.2f%%\"\\\n",
        "                  % (key, n_masks, i_iou, i_pla)\n",
        "            print(log)\n",
        "\n",
        "            # accumulate all image ious\n",
        "            s_iou += i_iou\n",
        "            \n",
        "            if self.plot:\n",
        "                self.evaluate(key, image)\n",
        "\n",
        "        n_test = len(self.test_keys)\n",
        "        m_iou = s_iou / n_test\n",
        "        #load miou and mpla history\n",
        "        if os.path.isfile(\"miou_history.npy\"):\n",
        "            self.miou_history = np.load(\"miou_history.npy\")\n",
        "            self.miou = np.amax(self.miou_history) \n",
        "        if os.path.isfile(\"mpla_history.npy\"):\n",
        "            self.mpla_history = np.load(\"mpla_history.npy\")\n",
        "\n",
        "        self.miou_history = np.append(self.miou_history, m_iou)\n",
        "        np.save(\"miou_history.npy\", self.miou_history)\n",
        "        m_pla = s_pla / n_test\n",
        "\n",
        "        self.mpla_history = np.append(self.mpla_history, m_pla)\n",
        "        np.save(\"mpla_history.npy\", self.mpla_history)\n",
        "\n",
        "        if m_iou > self.miou:\n",
        "            log = \"\\nOld best mIoU=%0.4f, New best mIoU=%0.4f, Pixel level accuracy=%0.2f%%\"\\\n",
        "                    % (self.miou, m_iou, m_pla)\n",
        "            print(log)\n",
        "            self.miou = m_iou\n",
        "            print(\"Saving weights... %s\" % self.weights_path)\n",
        "            #SAVE WEIGHTS FUNCTION FOR PYTORCH\n",
        "            torch.save({\n",
        "                'epoch': self.epochs,\n",
        "                'model_state_dict': self.fcn.state_dict(),\n",
        "                'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "                'loss': self.total_loss\n",
        "            }, self.weights_path)\n",
        "            \n",
        "        else:\n",
        "            log = \"\\nCurrent mIoU=%0.4f, Pixel level accuracy=%0.2f%%\"\\\n",
        "                    % (m_iou, m_pla)\n",
        "\n",
        "            print(log)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF8uYRNpg8We",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7721f19-5caa-495e-97b9-49af230ef946"
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "fcn = FCN(train = True, batch_size = 5, epochs = 40, \n",
        "          save = True, restore_weights_path = \"3layer-drinks-best-iou.h5\", evaluate = True, device = 'cuda')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules:  <bound method Module.modules of build_fcn(\n",
            "  (backbone): resnet_v1(\n",
            "    (res_x): ModuleList(\n",
            "      (0): resnet_layer(\n",
            "        (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(2, 2))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (res_y): ModuleList(\n",
            "      (0): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (2): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (3): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (4): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (5): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (6): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (7): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (8): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (9): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (10): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (11): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (12): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (13): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (14): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (15): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (16): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (17): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (18): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (19): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (20): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (21): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (22): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (23): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (24): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (25): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (26): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (27): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (28): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (29): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (30): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (31): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (32): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (33): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (34): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (35): resnet_layer(\n",
            "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (feat_pyramid): features_pyramid(\n",
            "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (layers): ModuleList(\n",
            "        (0): conv_layer(\n",
            "          (conv): Conv2d(16, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        )\n",
            "        (1): conv_layer(\n",
            "          (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layers): ModuleList(\n",
            "    (0): conv_layer(\n",
            "      (conv): Conv2d(16, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (1): conv_layer(\n",
            "      (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "    (2): conv_layer(\n",
            "      (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    )\n",
            "  )\n",
            "  (upsample): ModuleList(\n",
            "    (0): Upsample(scale_factor=2.0, mode=bilinear)\n",
            "    (1): Upsample(scale_factor=4.0, mode=bilinear)\n",
            "    (2): Upsample(scale_factor=8.0, mode=bilinear)\n",
            "  )\n",
            "  (tconv1): tconv_layer(\n",
            "    (tconv): ConvTranspose2d(784, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (tconv2): tconv_layer(\n",
            "    (tconv): ConvTranspose2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (tconv3): ConvTranspose2d(256, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            ")>\n",
            "Weights filename: 3layer-drinks-best-iou.h5\n",
            "Loaded data/drinks/segmentation_test.npy\n",
            "Loading weights: /content/drive/My Drive/Segmentation/weights/3layer-drinks-best-iou.h5\n",
            "Restored weights.\n",
            "# of classes 4\n",
            "Batch size: 5\n",
            "Training network....\n",
            "data length: 200\n",
            "Epochs done: 40\n",
            "Done training!\n",
            "Saving weights... /content/drive/My Drive/Segmentation/weights/3layer-drinks-best-iou.h5\n",
            "Weights are saved.\n",
            "Evaluating..\n",
            "Loading img: 0010050.jpg\n",
            "0010050.jpg: 4 objs, miou=0.8415 ,pla=96.97%\n",
            "Loading img: 0010000.jpg\n",
            "0010000.jpg: 4 objs, miou=0.7831 ,pla=94.30%\n",
            "Loading img: 0010001.jpg\n",
            "0010001.jpg: 4 objs, miou=0.7526 ,pla=92.89%\n",
            "Loading img: 0010002.jpg\n",
            "0010002.jpg: 4 objs, miou=0.9030 ,pla=98.30%\n",
            "Loading img: 0010003.jpg\n",
            "0010003.jpg: 4 objs, miou=0.9250 ,pla=97.91%\n",
            "Loading img: 0010004.jpg\n",
            "0010004.jpg: 4 objs, miou=0.9398 ,pla=98.42%\n",
            "Loading img: 0010005.jpg\n",
            "0010005.jpg: 4 objs, miou=0.8361 ,pla=96.33%\n",
            "Loading img: 0010006.jpg\n",
            "0010006.jpg: 2 objs, miou=0.9220 ,pla=95.71%\n",
            "Loading img: 0010007.jpg\n",
            "0010007.jpg: 3 objs, miou=0.8201 ,pla=92.85%\n",
            "Loading img: 0010008.jpg\n",
            "0010008.jpg: 3 objs, miou=0.9571 ,pla=98.15%\n",
            "Loading img: 0010009.jpg\n",
            "0010009.jpg: 3 objs, miou=0.9339 ,pla=98.61%\n",
            "Loading img: 0010010.jpg\n",
            "0010010.jpg: 4 objs, miou=0.8965 ,pla=97.85%\n",
            "Loading img: 0010011.jpg\n",
            "0010011.jpg: 4 objs, miou=0.8823 ,pla=97.14%\n",
            "Loading img: 0010012.jpg\n",
            "0010012.jpg: 4 objs, miou=0.8495 ,pla=95.56%\n",
            "Loading img: 0010013.jpg\n",
            "0010013.jpg: 4 objs, miou=0.8870 ,pla=96.85%\n",
            "Loading img: 0010014.jpg\n",
            "0010014.jpg: 4 objs, miou=0.8087 ,pla=94.29%\n",
            "Loading img: 0010015.jpg\n",
            "0010015.jpg: 4 objs, miou=0.7925 ,pla=93.16%\n",
            "Loading img: 0010016.jpg\n",
            "0010016.jpg: 4 objs, miou=0.6747 ,pla=89.18%\n",
            "Loading img: 0010017.jpg\n",
            "0010017.jpg: 3 objs, miou=0.8867 ,pla=96.23%\n",
            "Loading img: 0010018.jpg\n",
            "0010018.jpg: 3 objs, miou=0.8580 ,pla=94.57%\n",
            "Loading img: 0010019.jpg\n",
            "0010019.jpg: 3 objs, miou=0.8065 ,pla=93.24%\n",
            "Loading img: 0010020.jpg\n",
            "0010020.jpg: 3 objs, miou=0.8493 ,pla=95.79%\n",
            "Loading img: 0010021.jpg\n",
            "0010021.jpg: 3 objs, miou=0.6942 ,pla=87.01%\n",
            "Loading img: 0010022.jpg\n",
            "0010022.jpg: 3 objs, miou=0.8949 ,pla=97.58%\n",
            "Loading img: 0010023.jpg\n",
            "0010023.jpg: 3 objs, miou=0.7828 ,pla=93.43%\n",
            "Loading img: 0010024.jpg\n",
            "0010024.jpg: 4 objs, miou=0.9250 ,pla=97.43%\n",
            "Loading img: 0010025.jpg\n",
            "0010025.jpg: 4 objs, miou=0.8907 ,pla=96.63%\n",
            "Loading img: 0010026.jpg\n",
            "0010026.jpg: 4 objs, miou=0.8122 ,pla=96.34%\n",
            "Loading img: 0010027.jpg\n",
            "0010027.jpg: 4 objs, miou=0.8987 ,pla=97.89%\n",
            "Loading img: 0010028.jpg\n",
            "0010028.jpg: 4 objs, miou=0.9271 ,pla=98.21%\n",
            "Loading img: 0010029.jpg\n",
            "0010029.jpg: 4 objs, miou=0.8946 ,pla=97.09%\n",
            "Loading img: 0010030.jpg\n",
            "0010030.jpg: 4 objs, miou=0.8599 ,pla=98.10%\n",
            "Loading img: 0010031.jpg\n",
            "0010031.jpg: 4 objs, miou=0.9312 ,pla=98.73%\n",
            "Loading img: 0010032.jpg\n",
            "0010032.jpg: 3 objs, miou=0.7227 ,pla=93.91%\n",
            "Loading img: 0010033.jpg\n",
            "0010033.jpg: 3 objs, miou=0.7590 ,pla=96.31%\n",
            "Loading img: 0010034.jpg\n",
            "0010034.jpg: 4 objs, miou=0.8738 ,pla=97.16%\n",
            "Loading img: 0010035.jpg\n",
            "0010035.jpg: 4 objs, miou=0.8829 ,pla=98.12%\n",
            "Loading img: 0010036.jpg\n",
            "0010036.jpg: 3 objs, miou=0.8964 ,pla=97.11%\n",
            "Loading img: 0010037.jpg\n",
            "0010037.jpg: 3 objs, miou=0.8704 ,pla=96.40%\n",
            "Loading img: 0010038.jpg\n",
            "0010038.jpg: 3 objs, miou=0.9144 ,pla=97.31%\n",
            "Loading img: 0010039.jpg\n",
            "0010039.jpg: 3 objs, miou=0.9109 ,pla=97.19%\n",
            "Loading img: 0010040.jpg\n",
            "0010040.jpg: 4 objs, miou=0.9323 ,pla=98.54%\n",
            "Loading img: 0010041.jpg\n",
            "0010041.jpg: 4 objs, miou=0.8962 ,pla=98.32%\n",
            "Loading img: 0010042.jpg\n",
            "0010042.jpg: 3 objs, miou=0.8863 ,pla=96.24%\n",
            "Loading img: 0010043.jpg\n",
            "0010043.jpg: 3 objs, miou=0.9043 ,pla=96.29%\n",
            "Loading img: 0010044.jpg\n",
            "0010044.jpg: 3 objs, miou=0.8221 ,pla=94.69%\n",
            "Loading img: 0010045.jpg\n",
            "0010045.jpg: 3 objs, miou=0.8765 ,pla=96.67%\n",
            "Loading img: 0010046.jpg\n",
            "0010046.jpg: 3 objs, miou=0.8309 ,pla=97.10%\n",
            "Loading img: 0010047.jpg\n",
            "0010047.jpg: 4 objs, miou=0.8526 ,pla=97.87%\n",
            "Loading img: 0010048.jpg\n",
            "0010048.jpg: 4 objs, miou=0.9319 ,pla=99.04%\n",
            "Loading img: 0010049.jpg\n",
            "0010049.jpg: 4 objs, miou=0.7750 ,pla=95.51%\n",
            "\n",
            "Current mIoU=0.8599, Pixel level accuracy=96.21%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDre9MeEg8Wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fcn.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWeTquZgg8Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"data/drinks/0000107.jpg\"\n",
        "\n",
        "fcn.evaluate(imagefile = path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etM_LIcFg8W0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}